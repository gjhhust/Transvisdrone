Overriding model.yaml nc=1 with nc=8
                 from  n    params  module                                  arguments
  0                -1  1      7040  models.common.Conv                      [3, 64, 6, 2, 2]
  1                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]
  2                -1  3    156928  models.common.C3                        [128, 128, 3]
  3                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]
  4                -1  6   1118208  models.common.C3                        [256, 256, 6]
  5                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]
  6                -1  9   6433792  models.common.C3                        [512, 512, 9]
  7                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]
  8                -1  3   9971712  models.common.C3                        [1024, 1024, 3]
  9                -1  1   2624512  models.common.SPPF                      [1024, 1024, 5]
 10                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]
 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 12           [-1, 6]  1         0  models.common.Concat                    [1]
 13                -1  3   2757632  models.common.C3                        [1024, 512, 3, False]
 14                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]
 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']
 16           [-1, 4]  1         0  models.common.Concat                    [1]
 17                -1  3    690688  models.common.C3                        [512, 256, 3, False]
 18                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]
 19          [-1, 14]  1         0  models.common.Concat                    [1]
 20                -1  3   2495488  models.common.C3                        [512, 512, 3, False]
 21                -1  1   2360320  models.common.Conv                      [512, 512, 3, 2]
 22          [-1, 10]  1         0  models.common.Concat                    [1]
 23                -1  3   9971712  models.common.C3                        [1024, 1024, 3, False]
/data1/jiahaoguo/miniconda3/envs/transvisdrone/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
 24      [17, 20, 23]  1  33249389  models.yolo.Detect                      [8, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024], 5]
creating temporal trans 5, [256, 512, 1024]
Model Summary: 565 layers, 79355373 parameters, 79355373 gradients
Transferred 684/691 items from runs/train/NPS/image_size_1280_temporal_YOLO5l_5_frames_NPS_end_to_end_skip_0/weights/best.pt, excluding ['anchor']
Scaled weight_decay = 0.00036
[34m[1moptimizer:[39m[22m Adam with parameter groups 101 weight, 140 weight (no decay), 134 bias
[34m[1malbumentations: [39m[22mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.3, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.3, clip_limit=(1, 4.0), tile_grid_size=(8, 8)), RandomBrightnessContrast(always_apply=False, p=0.3, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.2, 0.2), brightness_by_max=True)
[34m[1mtrain: [39m[22mScanning '/data1/jiahaoguo/dataset/UAVTOD_1/all/yolo.cache' images and labels... 7426 found, 0 missing, 7 empty, 0
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0824Part2_1/0000020.png: 2 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000062.png: 28 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000063.png: 28 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000064.png: 28 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000066.png: 96 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000067.png: 69 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000074.png: 23 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000078.png: 20 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000080.png: 19 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000062.png: 43 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000063.png: 42 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000064.png: 42 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000066.png: 168 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000067.png: 126 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000074.png: 41 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000078.png: 38 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000080.png: 38 duplicate labels removed
[34m[1mval: [39m[22mScanning '/data1/jiahaoguo/dataset/UAVTOD_1/all/yolo.cache' images and labels... 7426 found, 0 missing, 7 empty, 0 co
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0824Part2_1/0000020.png: 2 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000062.png: 28 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000063.png: 28 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000064.png: 28 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000066.png: 96 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000067.png: 69 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000074.png: 23 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000078.png: 20 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_0/0000080.png: 19 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000062.png: 43 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000063.png: 42 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000064.png: 42 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000066.png: 168 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000067.png: 126 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000074.png: 41 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000078.png: 38 duplicate labels removed
[34m[1mtrain: [39m[22mWARNING: /data1/jiahaoguo/dataset/UAVTOD_1/images/./UAVTOD_DJI0825Part31_1/0000080.png: 38 duplicate labels removed
WARNING: --img-size 920 must be multiple of max stride 32, updating to 928
Frame wise augmentation set to 1
Shuffling indices because training
data loader shuffle True
Frame wise augmentation set to 1
data loader shuffle False
Number of val loader workers 2
Plotting labels...
Image sizes 928 train, 928 val
Using 0 dataloader workers
Logging results to [1mruns/train/USVID/visualization_5_frames
Starting training for 50 epochs...
     Epoch   gpu_mem       box       obj       cls    labels  img_size
  0%|                                                                                            | 0/7419 [00:00<?, ?it/s]
  0%|                                                                                            | 0/7419 [00:01<?, ?it/s]
Traceback (most recent call last):
  File "train.py", line 704, in <module>
    main(opt)
  File "train.py", line 601, in main
    train(opt.hyp, opt, device, callbacks)
  File "train.py", line 387, in train
    pred = model(imgs)  # forward
  File "/data1/jiahaoguo/miniconda3/envs/transvisdrone/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data1/jiahaoguo/TransVisDrone/models/yolo.py", line 146, in forward
    return self._forward_once(x, profile, visualize)  # single-scale inference, train
  File "/data1/jiahaoguo/TransVisDrone/models/yolo.py", line 169, in _forward_once
    x = m(x)  # run
  File "/data1/jiahaoguo/miniconda3/envs/transvisdrone/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data1/jiahaoguo/TransVisDrone/models/yolo.py", line 67, in forward
    x[i] = self.temporaltransformers[i]( x[i] )
  File "/data1/jiahaoguo/miniconda3/envs/transvisdrone/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data1/jiahaoguo/TransVisDrone/models/video_swin_transformer.py", line 408, in forward
    x = self.tr(x)
  File "/data1/jiahaoguo/miniconda3/envs/transvisdrone/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data1/jiahaoguo/miniconda3/envs/transvisdrone/lib/python3.8/site-packages/torch/nn/modules/container.py", line 141, in forward
    input = module(input)
  File "/data1/jiahaoguo/miniconda3/envs/transvisdrone/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data1/jiahaoguo/TransVisDrone/models/video_swin_transformer.py", line 347, in forward
    x = self.forward_part1(x, mask_matrix)
  File "/data1/jiahaoguo/TransVisDrone/models/video_swin_transformer.py", line 313, in forward_part1
    attn_windows = self.attn(x_windows, mask=attn_mask)  # B*nW, Wd*Wh*Ww, C
  File "/data1/jiahaoguo/miniconda3/envs/transvisdrone/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1102, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data1/jiahaoguo/TransVisDrone/models/video_swin_transformer.py", line 205, in forward
    attn = q @ k.transpose(-2, -1)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.90 GiB total capacity; 10.43 GiB already allocated; 10.94 MiB free; 11.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF